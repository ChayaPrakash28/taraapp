<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Talk to Tara ‚ú®</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: linear-gradient(180deg,#fff8f0 0%, #fff1f7 100%);
      --pink:#FF7AA2; --blue:#7AD7FF; --muted:#6b6b6b;
    }
    * { box-sizing:border-box; font-family:"Poppins","Segoe UI",Roboto,sans-serif }
    body {
      margin:0; min-height:100vh; background:var(--bg);
      display:flex; align-items:center; justify-content:center; padding:24px;
    }
    .card {
      width:100%; max-width:420px; background:#fff; border-radius:18px;
      padding:22px; text-align:center; box-shadow:0 10px 30px rgba(0,0,0,0.06);
    }
    h1 { margin:0 0 6px; font-size:22px; color:#4b2c4a }
    p.lead { margin:0 0 18px; color:var(--muted); font-size:14px }
    .mic-wrap { position:relative; margin:6px auto 12px auto; width:220px; height:220px }
    .mic-button {
      width:180px; height:180px; border-radius:50%;
      background:linear-gradient(135deg,var(--pink),var(--blue));
      border:none; cursor:pointer; display:flex; align-items:center; justify-content:center;
      box-shadow:0 12px 30px rgba(122,215,255,0.18); transition:transform .12s;
    }
    .mic-button:active { transform:translateY(3px) }
    .mic-button.recording { animation:pulse 1s infinite }
    @keyframes pulse { 0%{transform:scale(1)}50%{transform:scale(1.06)}100%{transform:scale(1)} }
    .mic-icon { color:white; font-size:64px; filter:drop-shadow(0 4px 12px rgba(0,0,0,0.12)) }
    .status { margin-top:12px; font-size:15px; color:#4b2c4a; display:flex; align-items:center; justify-content:center; gap:10px }
    .info { margin-top:12px; color:var(--muted); font-size:13px; min-height:18px }
  </style>
</head>
<body>
  <div class="card">
    <h1>Talk to Tara üåü</h1>
    <p class="lead">Tap once to start ‚Äî say ‚ÄúHey Tara‚Äù to wake her up. She‚Äôll keep listening until you say ‚ÄúStop Tara‚Äù.</p>

    <div class="mic-wrap">
      <button id="micBtn" class="mic-button" aria-pressed="false">
        <svg class="mic-icon" viewBox="0 0 24 24" width="64" height="64">
          <path fill="currentColor" d="M12 14a3 3 0 0 0 3-3V5a3 3 0 0 0-6 0v6a3 3 0 0 0 3 3z"/>
          <path fill="currentColor" d="M19 11a7 7 0 0 1-14 0h2a5 5 0 0 0 10 0h2z" opacity="0.12"/>
        </svg>
      </button>
    </div>

    <div class="status"><div id="statusText">Tap to start üé§</div></div>
    <div class="info" id="info">Tara listens only in English.</div>
    <audio id="responseAudio" preload="auto" hidden></audio>
  </div>

<script>
const UPLOAD_URL = 'https://taraapp-c1qt.onrender.com/upload_audio';
const MAX_RECORD_MS = 8000;
const SILENCE_THRESHOLD = 0.01;
const SILENCE_LEAD_MS = 850;

const micBtn = document.getElementById('micBtn');
const statusText = document.getElementById('statusText');
const info = document.getElementById('info');
const responseAudio = document.getElementById('responseAudio');

let mediaRecorder, audioChunks = [], audioContext, analyser, sourceNode;
let recording = false, taraActive = false, fallbackStopTimer;

// convo tracking
let convoCount = 0;
const MAX_CONVO = 10;
const TERMINATION_WORDS = ["stop tara", "bye tara", "goodbye tara"];

navigator.mediaDevices.getUserMedia({ audio: true })
  .then(() => console.log('üé§ Mic permission granted'))
  .catch(err => { console.error('Mic error:', err); info.textContent = 'Mic permission needed'; });

function setRecordingUI(isRec) {
  recording = isRec;
  micBtn.classList.toggle('recording', isRec);
  statusText.textContent = isRec ? (taraActive ? 'Listening...' : 'Listening (say "Hey Tara")') : (taraActive ? 'Processing...' : 'Tap to start üé§');
}

async function startRecording() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    audioChunks = [];
    mediaRecorder = new MediaRecorder(stream);
    mediaRecorder.ondataavailable = e => { if (e.data.size > 0) audioChunks.push(e.data); };
    mediaRecorder.onstop = processRecording;
    mediaRecorder.start();

    setupSilenceDetection(stream);
    setRecordingUI(true);
    fallbackStopTimer = setTimeout(stopRecording, MAX_RECORD_MS);
  } catch (err) {
    console.error('Mic start error:', err);
    statusText.textContent = 'üéôÔ∏è Mic unavailable';
  }
}

function stopRecording() {
  if (mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop();
  teardownSilenceDetection();
  clearTimeout(fallbackStopTimer);
  setRecordingUI(false);
}

async function processRecording() {
  if (!audioChunks.length) return;
  statusText.textContent = 'Uploading...';
  const blob = new Blob(audioChunks, { type: 'audio/webm' });
  const form = new FormData();
  form.append('audio', blob, 'talk.webm');

  try {
    const res = await fetch(UPLOAD_URL, { method: 'POST', body: form });
    const type = res.headers.get('content-type') || '';
    if (type.includes('audio')) {
      const audioBlob = await res.blob();
      playAudio(audioBlob);
    } else {
      const j = await res.json();

      // track transcript for termination / max convo
      if (j && j.transcript) {
        const text = j.transcript.toLowerCase();
        convoCount++;
        if (
          convoCount >= MAX_CONVO ||
          TERMINATION_WORDS.some(w => text.includes(w))
        ) {
          taraActive = false;
          statusText.textContent = "üåô Tara stopped";
          return; // stop loop
        }
      }

      if (j.status === 'activated') {
        taraActive = true;
        convoCount = 0;
        statusText.textContent = '‚ú® Tara activated';
        setTimeout(startRecording, 400);
      } else if (j.status === 'terminated') {
        taraActive = false;
        statusText.textContent = 'üåô Tara sleeping';
      } else if (taraActive) {
        setTimeout(startRecording, 300);
      } else {
        statusText.textContent = 'Say "Hey Tara"';
      }
    }
  } catch (err) {
    console.error('Upload error:', err);
    if (taraActive) setTimeout(startRecording, 800);
  } finally {
    audioChunks = [];
  }
}

function playAudio(blob) {
  responseAudio.src = URL.createObjectURL(blob);
  responseAudio.play()
    .then(() => statusText.textContent = 'Tara speaking üéß')
    .catch(() => console.warn('Playback failed'));
  responseAudio.onended = () => {
    if (taraActive) {
      statusText.textContent = "üëÇ Waiting for your voice...";
      waitForUserVoice().then(() => {
        if (taraActive) startRecording();
      });
    }
  };
}

// üëÇ helper: wait until voice is detected before recording
function waitForUserVoice() {
  return new Promise(async (resolve) => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const ctx = new (window.AudioContext || window.webkitAudioContext)();
      const analyser = ctx.createAnalyser();
      const src = ctx.createMediaStreamSource(stream);
      src.connect(analyser);
      analyser.fftSize = 512;
      const data = new Uint8Array(analyser.frequencyBinCount);

      function detect() {
        analyser.getByteTimeDomainData(data);
        let sum = 0;
        for (let i = 0; i < data.length; i++) {
          const v = (data[i] - 128) / 128;
          sum += v * v;
        }
        const rms = Math.sqrt(sum / data.length);
        if (rms > 0.02) { // üîß adjust threshold if too sensitive
          ctx.close();
          stream.getTracks().forEach(t => t.stop());
          resolve();
          return;
        }
        requestAnimationFrame(detect);
      }
      detect();
    } catch (err) {
      console.error("Voice wait error:", err);
      resolve(); // fallback: just continue
    }
  });
}

function setupSilenceDetection(stream) {
  audioContext = new (window.AudioContext || window.webkitAudioContext)();
  analyser = audioContext.createAnalyser();
  sourceNode = audioContext.createMediaStreamSource(stream);
  sourceNode.connect(analyser);
  analyser.fftSize = 512;
  const data = new Uint8Array(analyser.frequencyBinCount);
  let lastLoud = Date.now();

  function detect() {
    analyser.getByteTimeDomainData(data);
    let sum = 0;
    for (let i = 0; i < data.length; i++) {
      const v = (data[i] - 128) / 128;
      sum += v * v;
    }
    const rms = Math.sqrt(sum / data.length);
    if (rms > SILENCE_THRESHOLD) lastLoud = Date.now();
    else if (Date.now() - lastLoud > SILENCE_LEAD_MS) stopRecording();
    if (recording) requestAnimationFrame(detect);
  }
  requestAnimationFrame(detect);
}

function teardownSilenceDetection() {
  if (sourceNode) sourceNode.disconnect();
  if (analyser) analyser.disconnect();
  if (audioContext) audioContext.close();
}

// üü¢ Tap once to start everything
micBtn.addEventListener('click', () => {
  if (!recording) {
    taraActive = true;
    convoCount = 0;
    startRecording();
  } else {
    taraActive = false;
    statusText.textContent = "‚èπÔ∏è Stopped manually";
  }
});
</script>
</body>
</html>
