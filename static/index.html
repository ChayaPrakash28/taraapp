<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Talk to Tara âœ¨</title>
  <link rel="manifest" href="/static/manifest.json">
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root {
      --bg: linear-gradient(180deg,#fff8f0 0%, #fff1f7 100%);
      --pink:#FF7AA2; --blue:#7AD7FF; --muted:#6b6b6b;
    }
    * { box-sizing:border-box; font-family:"Poppins","Segoe UI",Roboto,sans-serif }
    body {
      margin:0; min-height:100vh; background:var(--bg);
      display:flex; align-items:center; justify-content:center; padding:24px;
    }
    .card {
      width:100%; max-width:420px; background:#fff; border-radius:18px;
      padding:22px; text-align:center; box-shadow:0 10px 30px rgba(0,0,0,0.06);
    }
    h1 { margin:0 0 6px; font-size:22px; color:#4b2c4a }
    p.lead { margin:0 0 18px; color:var(--muted); font-size:14px }
    .mic-wrap { position:relative; margin:6px auto 12px auto; width:220px; height:220px }
    .mic-button {
      width:180px; height:180px; border-radius:50%;
      background:linear-gradient(135deg,var(--pink),var(--blue));
      border:none; cursor:pointer; display:flex; align-items:center; justify-content:center;
      box-shadow:0 12px 30px rgba(122,215,255,0.18); transition:transform .12s;
    }
    .mic-button:active { transform:translateY(3px) }
    .mic-button.recording { animation:pulse 1s infinite }
    @keyframes pulse { 0%{transform:scale(1)}50%{transform:scale(1.06)}100%{transform:scale(1)} }
    .mic-icon { color:white; font-size:64px; filter:drop-shadow(0 4px 12px rgba(0,0,0,0.12)) }
    .status { margin-top:12px; font-size:15px; color:#4b2c4a; display:flex; align-items:center; justify-content:center; gap:10px }
    .info { margin-top:12px; color:var(--muted); font-size:13px; min-height:18px }
  </style>
</head>
<body>
  <div class="card" role="main" aria-labelledby="title">
    <h1 id="title">Talk to Tara ðŸŒŸ</h1>
    <p class="lead">Tap once, say "Hey Tara" â€” sheâ€™ll wake up and chat until you say "Stop Tara".</p>

    <div class="mic-wrap">
      <button id="micBtn" class="mic-button" aria-pressed="false" aria-label="Talk to Tara">
        <svg class="mic-icon" viewBox="0 0 24 24" width="64" height="64" aria-hidden="true">
          <path fill="currentColor" d="M12 14a3 3 0 0 0 3-3V5a3 3 0 0 0-6 0v6a3 3 0 0 0 3 3z"/>
          <path fill="currentColor" d="M19 11a7 7 0 0 1-14 0h2a5 5 0 0 0 10 0h2z" opacity="0.12"/>
        </svg>
      </button>
    </div>

    <div class="status" id="statusBar">
      <div id="statusText">Tap to talk ðŸŽ¤</div>
    </div>

    <div class="info" id="info">Tara listens only in English.</div>

    <audio id="responseAudio" style="display:none" preload="auto"></audio>
  </div>

<script>
/* ðŸŒ Backend URLs on Render */
const UPLOAD_URL = 'https://taraapp-c1qt.onrender.com/upload_audio';
const ASK_URL = 'https://taraapp-c1qt.onrender.com/ask';

/* ðŸŽ¤ Recording configuration */
const MAX_RECORD_MS = 8000;
const SILENCE_THRESHOLD = 0.01;
const SILENCE_LEAD_MS = 850;

const micBtn = document.getElementById('micBtn');
const statusText = document.getElementById('statusText');
const info = document.getElementById('info');
const responseAudio = document.getElementById('responseAudio');

let mediaRecorder = null;
let audioChunks = [];
let recording = false;
let audioContext, analyser, sourceNode;
let taraActive = false;
let fallbackStopTimer = null;

/* ðŸŸ¢ Request mic permission early (for WebView) */
navigator.mediaDevices.getUserMedia({ audio: true })
  .then(stream => console.log('ðŸŽ¤ Mic access granted'))
  .catch(err => {
    console.error('âŒ Mic access error:', err);
    info.textContent = 'Microphone permission denied. Please enable it in app settings.';
  });

function setRecordingUI(isRec){
  recording = isRec;
  micBtn.classList.toggle('recording', isRec);
  micBtn.setAttribute('aria-pressed', isRec ? 'true' : 'false');
  if (isRec) {
    statusText.textContent = taraActive ? 'Listening...' : 'Listening (waiting for "Hey Tara")';
    info.textContent = taraActive ? 'Tara is awake â€” say something!' : 'Say "Hey Tara" to activate';
  } else {
    statusText.textContent = taraActive ? 'Processing...' : 'Tap to talk ðŸŽ¤';
    info.textContent = taraActive ? 'Tara is thinking...' : 'Tara listens only in English.';
  }
}

async function startRecordingAuto(){
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    audioChunks = [];
    mediaRecorder = new MediaRecorder(stream);
    mediaRecorder.ondataavailable = e => { if(e.data && e.data.size > 0) audioChunks.push(e.data); };
    mediaRecorder.onstop = handleRecordingStopped;
    mediaRecorder.start();

    setupSilenceDetection(stream);
    setRecordingUI(true);
    fallbackStopTimer = setTimeout(() => {
      if(mediaRecorder && mediaRecorder.state !== 'inactive') stopRecording();
    }, MAX_RECORD_MS);
  } catch(err){
    console.error('ðŸŽ™ï¸ Mic error:', err);
    statusText.textContent = 'ðŸŽ™ï¸ Microphone access required';
  }
}

function stopRecording(){
  if(mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop();
  teardownSilenceDetection();
  clearTimeout(fallbackStopTimer);
  setRecordingUI(false);
}

async function handleRecordingStopped(){
  if (audioChunks.length === 0) {
    statusText.textContent = 'No audio captured â€” try again';
    return;
  }

  statusText.textContent = 'Uploading...';
  const blob = new Blob(audioChunks, { type: audioChunks[0].type || 'audio/webm' });
  const form = new FormData();
  form.append('audio', blob, 'talk.' + (blob.type.split('/')[1] || 'webm'));

  try {
    const res = await fetch(UPLOAD_URL, { method: 'POST', body: form });
    if(!res.ok){
      const txt = await res.text();
      console.error('Upload failed', res.status, txt);
      statusText.textContent = 'Upload failed âŒ';
      if(taraActive) setTimeout(startRecordingAuto, 350);
      return;
    }

    const contentType = (res.headers.get('content-type') || '').toLowerCase();
    if(contentType.includes('audio')){
      const audioBlob = await res.blob();
      playResponseAudio(audioBlob);
      return;
    } else {
      const j = await res.json();
      if (j.status === 'activated') {
        taraActive = true;
        statusText.textContent = 'âœ¨ Tara is awake!';
        setTimeout(startRecordingAuto, 300);
      } else if (j.status === 'terminated') {
        taraActive = false;
        statusText.textContent = 'ðŸŒ™ Tara went to sleep';
      } else if (j.status === 'ignored') {
        statusText.textContent = 'Say "Hey Tara" to start';
      } else {
        if (taraActive) setTimeout(startRecordingAuto, 300);
      }
    }
  } catch(err){
    console.error('âš ï¸ Upload/network error:', err);
    statusText.textContent = 'âš ï¸ Network or CORS error';
    if (taraActive) setTimeout(startRecordingAuto, 700);
  } finally {
    audioChunks = [];
  }
}

function playResponseAudio(blob){
  responseAudio.src = URL.createObjectURL(blob);
  responseAudio.style.display = 'block';
  responseAudio.play().catch(e => console.log('playback failed', e));
  statusText.textContent = 'Tara is talking ðŸŽ§';
  responseAudio.onended = () => {
    responseAudio.src = '';
    responseAudio.style.display = 'none';
    if (taraActive) setTimeout(startRecordingAuto, 300);
    else setRecordingUI(false);
  };
}

/* ðŸ”Š Silence detection */
function setupSilenceDetection(stream){
  try {
    audioContext = new (window.AudioContext || window.webkitAudioContext)();
    analyser = audioContext.createAnalyser();
    analyser.fftSize = 512;
    sourceNode = audioContext.createMediaStreamSource(stream);
    sourceNode.connect(analyser);
    const data = new Uint8Array(analyser.frequencyBinCount);
    let lastLoud = Date.now();

    function detect(){
      analyser.getByteTimeDomainData(data);
      let sum = 0;
      for(let i=0;i<data.length;i++){
        const v = (data[i]-128)/128;
        sum += v*v;
      }
      const rms = Math.sqrt(sum/data.length);

      if(rms > SILENCE_THRESHOLD){
        lastLoud = Date.now();
      } else if(Date.now() - lastLoud > SILENCE_LEAD_MS){
        if(mediaRecorder && mediaRecorder.state !== 'inactive'){
          stopRecording();
        }
      }
      if(recording) requestAnimationFrame(detect);
    }
    requestAnimationFrame(detect);
  } catch(err){
    console.warn('Silence detection failed', err);
  }
}

function teardownSilenceDetection(){
  try {
    if(sourceNode){ sourceNode.disconnect(); sourceNode=null; }
    if(analyser){ analyser.disconnect(); analyser=null; }
    if(audioContext){ audioContext.close(); audioContext=null; }
  } catch(e){}
}

/* ðŸŽ™ï¸ Event Listeners */
micBtn.addEventListener('click', () => {
  if(!recording) startRecordingAuto();
  else stopRecording();
});

micBtn.addEventListener('keydown', e => {
  if(e.key === ' ' || e.key === 'Enter'){ e.preventDefault(); micBtn.click(); }
});

window.addEventListener('beforeunload', () => {
  if(mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop();
});
</script>
</body>
</html>
